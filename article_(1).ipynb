{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W_IQQ-uzIW-o"
      },
      "outputs": [],
      "source": [
        "import zipfile as zf\n",
        "files = zf.ZipFile(\"/content/archive (1).zip\", 'r')\n",
        "files.extractall('my_data')\n",
        "files.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_your_image_data(image_dir):\n",
        "    image_files = os.listdir(image_dir)\n",
        "    image_data = []\n",
        "\n",
        "    for file_name in image_files:\n",
        "        image_path = os.path.join(image_dir, file_name)\n",
        "        image = cv2.imread(image_path)  # Load image using OpenCV\n",
        "        image_data.append(image)\n",
        "\n",
        "    return np.array(image_data)"
      ],
      "metadata": {
        "id": "f2H1HBgCWKqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/my_data'\n",
        "X_images = load_your_image_data(image_dir)"
      ],
      "metadata": {
        "id": "wYJggqZ0WTxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6bY9JX5AT86",
        "outputId": "1485fe98-e192-4e5d-c115-4097a2dc1fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[151, 202, 235],\n",
              "         [155, 206, 239],\n",
              "         [152, 205, 238],\n",
              "         ...,\n",
              "         [ 67,  76,  86],\n",
              "         [ 59,  68,  78],\n",
              "         [ 57,  66,  76]],\n",
              "\n",
              "        [[165, 218, 251],\n",
              "         [172, 225, 255],\n",
              "         [170, 223, 255],\n",
              "         ...,\n",
              "         [ 60,  68,  81],\n",
              "         [ 54,  63,  73],\n",
              "         [ 58,  67,  77]],\n",
              "\n",
              "        [[152, 205, 238],\n",
              "         [158, 211, 244],\n",
              "         [155, 209, 242],\n",
              "         ...,\n",
              "         [ 66,  74,  87],\n",
              "         [ 53,  61,  74],\n",
              "         [ 48,  56,  69]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[161, 181, 206],\n",
              "         [147, 167, 192],\n",
              "         [145, 163, 186],\n",
              "         ...,\n",
              "         [ 75,  67,  68],\n",
              "         [ 75,  67,  68],\n",
              "         [ 83,  75,  76]],\n",
              "\n",
              "        [[149, 169, 194],\n",
              "         [158, 178, 203],\n",
              "         [155, 175, 200],\n",
              "         ...,\n",
              "         [ 84,  76,  77],\n",
              "         [ 78,  70,  71],\n",
              "         [ 80,  72,  73]],\n",
              "\n",
              "        [[140, 160, 185],\n",
              "         [145, 165, 190],\n",
              "         [146, 166, 191],\n",
              "         ...,\n",
              "         [ 90,  82,  83],\n",
              "         [ 86,  78,  79],\n",
              "         [ 76,  68,  69]]],\n",
              "\n",
              "\n",
              "       [[[136, 146, 163],\n",
              "         [130, 140, 157],\n",
              "         [123, 133, 150],\n",
              "         ...,\n",
              "         [126, 136, 166],\n",
              "         [118, 126, 156],\n",
              "         [108, 116, 146]],\n",
              "\n",
              "        [[156, 166, 183],\n",
              "         [149, 159, 176],\n",
              "         [151, 161, 178],\n",
              "         ...,\n",
              "         [123, 133, 163],\n",
              "         [116, 127, 155],\n",
              "         [127, 138, 166]],\n",
              "\n",
              "        [[157, 170, 186],\n",
              "         [144, 157, 173],\n",
              "         [153, 163, 180],\n",
              "         ...,\n",
              "         [127, 139, 167],\n",
              "         [112, 124, 152],\n",
              "         [109, 121, 149]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[130, 142, 166],\n",
              "         [130, 142, 166],\n",
              "         [123, 135, 159],\n",
              "         ...,\n",
              "         [ 56,  51,  50],\n",
              "         [ 41,  36,  35],\n",
              "         [ 38,  33,  32]],\n",
              "\n",
              "        [[123, 134, 161],\n",
              "         [112, 123, 150],\n",
              "         [115, 126, 153],\n",
              "         ...,\n",
              "         [ 54,  49,  50],\n",
              "         [ 50,  45,  46],\n",
              "         [ 32,  27,  28]],\n",
              "\n",
              "        [[117, 128, 155],\n",
              "         [112, 123, 150],\n",
              "         [113, 124, 151],\n",
              "         ...,\n",
              "         [ 42,  37,  38],\n",
              "         [ 43,  38,  39],\n",
              "         [ 49,  44,  45]]],\n",
              "\n",
              "\n",
              "       [[[163, 217, 250],\n",
              "         [166, 220, 253],\n",
              "         [166, 219, 252],\n",
              "         ...,\n",
              "         [101, 109, 126],\n",
              "         [100, 108, 125],\n",
              "         [ 98, 106, 123]],\n",
              "\n",
              "        [[166, 220, 253],\n",
              "         [158, 211, 244],\n",
              "         [153, 206, 239],\n",
              "         ...,\n",
              "         [100, 108, 125],\n",
              "         [ 97, 105, 122],\n",
              "         [ 94, 102, 119]],\n",
              "\n",
              "        [[155, 208, 241],\n",
              "         [140, 191, 224],\n",
              "         [140, 191, 224],\n",
              "         ...,\n",
              "         [100, 108, 125],\n",
              "         [108, 116, 133],\n",
              "         [102, 110, 127]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[163, 193, 220],\n",
              "         [167, 197, 224],\n",
              "         [173, 203, 230],\n",
              "         ...,\n",
              "         [ 26,  25,  29],\n",
              "         [ 28,  27,  31],\n",
              "         [ 36,  35,  39]],\n",
              "\n",
              "        [[161, 190, 217],\n",
              "         [159, 188, 215],\n",
              "         [161, 190, 217],\n",
              "         ...,\n",
              "         [ 44,  39,  41],\n",
              "         [ 41,  36,  38],\n",
              "         [ 46,  41,  43]],\n",
              "\n",
              "        [[161, 190, 217],\n",
              "         [157, 186, 213],\n",
              "         [155, 184, 211],\n",
              "         ...,\n",
              "         [ 44,  39,  41],\n",
              "         [ 40,  35,  37],\n",
              "         [ 43,  38,  40]]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_your_crop_yield_data():\n",
        "    # Replace with your code to load actual crop yield data\n",
        "    # Make sure y_crop is an array of crop yield values\n",
        "    y_crop = np.array([100, 200, 150])\n",
        "    return y_crop"
      ],
      "metadata": {
        "id": "ofld5QICZaUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "bcsRdAlah_r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your own image data (replace with your actual image loading code)\n",
        "# Make sure X_images is an array of shape (num_samples, image_height, image_width, 3)\n",
        "# X_images = load_your_image_data()\n",
        "\n",
        "# Apply K-Means clustering for tree detection\n",
        "num_clusters = 2\n",
        "kmeans_images = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "\n",
        "# Flatten images and apply K-Means clustering\n",
        "X_flattened = X_images.reshape(-1, 3)\n",
        "tree_labels = kmeans_images.fit_predict(X_flattened)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRE9CtVDiISa",
        "outputId": "4fd88914-2961-41a0-cc1c-59cbe606a2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthesize a super-feature vector\n",
        "super_feature_vector = []\n",
        "\n",
        "for i in range(3):\n",
        "    features = []\n",
        "\n",
        "    # Convert image to the appropriate data type\n",
        "    scaled_image = (X_images[i] * 255).astype(np.uint8)\n",
        "\n",
        "    # Convert scaled image to grayscale\n",
        "    gray_image = cv2.cvtColor(scaled_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Apply threshold to create a binary image\n",
        "    ret, binary_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Extract geometric features (e.g., bounding box dimensions)\n",
        "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours:\n",
        "        x, y, w, h = cv2.boundingRect(contours[0])\n",
        "        features.extend([w, h])\n",
        "    else:\n",
        "        features.extend([0, 0])\n",
        "\n",
        "    super_feature_vector.append(features)\n",
        "\n",
        "super_feature_vector = np.array(super_feature_vector)"
      ],
      "metadata": {
        "id": "oM5uNSbKiOTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_feature_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk0q9tHgD4RM",
        "outputId": "f02c2987-e2bd-4c55-8128-98871cee05fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 1],\n",
              "       [1, 1],\n",
              "       [2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your actual crop yield data (replace with your data loading code)\n",
        "# Make sure y_crop is an array of crop yield values\n",
        "y_crop = load_your_crop_yield_data()"
      ],
      "metadata": {
        "id": "ZaFMOaDWiTQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(super_feature_vector, y_crop, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "s6A5vKqEiXL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a linear regression model for crop estimation\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "S_EJaVuIicmn",
        "outputId": "80b7c057-2743-43b3-e2d4-b2e7f3d95cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict crop yield on the testing set\n",
        "y_pred = regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "-JMpYYSXikLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the regression model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE2VxUVTWZZB",
        "outputId": "db37efde-eb10-475d-dbb6-83561d7bff12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 2499.9999999999945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8BN9moha3va",
        "outputId": "f2c6f341-8597-44ad-8b7e-6a5de202447b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([200.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeqeiL2ka_sF",
        "outputId": "8798d526-c96c-4c14-ce26-56b7cf190321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([150])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "hTfj8nWecfgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img= '/content/my_data/DJI_0279.JPG'"
      ],
      "metadata": {
        "id": "_UUzHjmKevyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Load an image\n",
        "image_path = \"/content/my_data/DJI_0280.JPG\"\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Get the image dimensions (height and width)\n",
        "image_height, image_width, _ = image.shape"
      ],
      "metadata": {
        "id": "rUhud6Sley91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic image data (replace with your actual image data)\n",
        "num_samples = 100\n",
        "image_height = 256\n",
        "image_width = 256\n",
        "X_images = np.random.rand(num_samples, image_height, image_width, 3)\n",
        "\n",
        "# Apply K-Means clustering for tree detection\n",
        "num_clusters = 2\n",
        "kmeans_images = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "\n",
        "# Flatten images and apply K-Means clustering\n",
        "X_flattened = X_images.reshape(-1, 3)\n",
        "tree_labels = kmeans_images.fit_predict(X_flattened)\n",
        "\n",
        "# Synthesize a super-feature vector\n",
        "super_feature_vector = []\n",
        "\n",
        "# ...\n",
        "for i in range(num_samples):\n",
        "    features = []\n",
        "\n",
        "    # Convert image to the appropriate data type\n",
        "    scaled_image = (X_images[i] * 255).astype(np.uint8)\n",
        "\n",
        "    # Convert scaled image to grayscale\n",
        "    gray_image = cv2.cvtColor(scaled_image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Apply threshold to create a binary image\n",
        "    ret, binary_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Extract geometric features (e.g., bounding box dimensions)\n",
        "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if contours:\n",
        "        x, y, w, h = cv2.boundingRect(contours[0])\n",
        "        features.extend([w, h])\n",
        "    else:\n",
        "        features.extend([0, 0])\n",
        "\n",
        "    super_feature_vector.append(features)\n",
        "\n",
        "super_feature_vector = np.array(super_feature_vector)\n",
        "# ...\n",
        "\n",
        "\n",
        "# Generate synthetic crop yield data (replace with your actual data)\n",
        "y_crop = np.random.rand(num_samples) * 1000\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(super_feature_vector, y_crop, test_size=0.2, random_state=0)\n",
        "\n",
        "# Train a linear regression model for crop estimation\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict crop yield on the testing set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the regression model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddlb6ULFURef",
        "outputId": "65349f30-5c7e-4a8b-c021-ceb5b525de96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 84820.2299941559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = '/content/my_data'\n",
        "X_images = load_your_image_data(image_dir)"
      ],
      "metadata": {
        "id": "rOMYGcpQkhv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "# Generate synthetic image data (replace with your actual image data loading code)\n",
        "num_samples = 100\n",
        "image_height = 256\n",
        "image_width = 256\n",
        "X_images = np.random.rand(num_samples, image_height, image_width, 3)\n",
        "\n",
        "# Generate synthetic crop yield data (replace with your actual crop yield data loading code)\n",
        "y_crop = np.random.rand(num_samples) * 1000\n",
        "\n",
        "# Apply K-Means clustering for tree detection\n",
        "num_clusters = 2\n",
        "kmeans_images = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "\n",
        "# Flatten images and apply K-Means clustering\n",
        "X_flattened = X_images.reshape(-1, 3)\n",
        "tree_labels = kmeans_images.fit_predict(X_flattened)\n",
        "\n",
        "# Convert tree labels to one-hot encoding\n",
        "tree_labels_onehot = np.eye(num_clusters)[tree_labels]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_images, tree_labels_onehot, test_size=0.2, random_state=0)\n",
        "\n",
        "# Train the classification model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(image_height, image_width, 3)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(num_clusters, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_cls, y_train_cls, epochs=10, batch_size=32, validation_data=(X_test_cls, y_test_cls))\n"
      ],
      "metadata": {
        "id": "V936BfEmo5PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_foreground = [1, 0, 1]\n",
        "y_regression = [20, 15, 30]"
      ],
      "metadata": {
        "id": "LMPiMQIhV3VS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "# Step 1: Data loading and preprocessing\n",
        "\n",
        "# Path to the folder containing the images\n",
        "image_folder = '/content/my_data'\n",
        "\n",
        "# Load and preprocess images\n",
        "images = []\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(\".JPG\") or filename.endswith(\".png\"):\n",
        "        img = cv2.imread(os.path.join(image_folder, filename))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        img = cv2.resize(img, (224, 224))  # Resize images to a consistent size\n",
        "        images.append(img)\n",
        "\n",
        "# Step 2: K-Means clustering for enhanced segmentation\n",
        "\n",
        "def kmeans_segmentation(image, num_clusters=3):\n",
        "    reshaped_img = image.reshape((-1, 3))\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(reshaped_img)\n",
        "    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
        "    segmented_img = segmented_img.reshape(image.shape)\n",
        "    return segmented_img\n",
        "\n",
        "segmented_images = [kmeans_segmentation(img) for img in images]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovKr1XNy5Wz7",
        "outputId": "d87e9f71-fdeb-4c12-fd67-19bd4d3e6f76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(segmented_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCRyZIa75YtW",
        "outputId": "5347d81a-4b83-446e-9952-481e28037763"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[225.78031496, 197.77349081, 157.55728346],\n",
              "         [225.78031496, 197.77349081, 157.55728346],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         ...,\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         [151.23649682, 127.86735593, 104.11924521]],\n",
              "\n",
              "        [[225.78031496, 197.77349081, 157.55728346],\n",
              "         [225.78031496, 197.77349081, 157.55728346],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         ...,\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [151.23649682, 127.86735593, 104.11924521]],\n",
              "\n",
              "        [[225.78031496, 197.77349081, 157.55728346],\n",
              "         [225.78031496, 197.77349081, 157.55728346],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         ...,\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[151.23649682, 127.86735593, 104.11924521],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         ...,\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327]],\n",
              "\n",
              "        [[151.23649682, 127.86735593, 104.11924521],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         [225.78031496, 197.77349081, 157.55728346],\n",
              "         ...,\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327]],\n",
              "\n",
              "        [[225.78031496, 197.77349081, 157.55728346],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         [151.23649682, 127.86735593, 104.11924521],\n",
              "         ...,\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327],\n",
              "         [ 59.37487843,  53.54559737,  48.68781327]]],\n",
              "\n",
              "\n",
              "       [[[157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         ...,\n",
              "         [ 69.50700865,  62.57702058,  55.79331942],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [230.75887499, 204.00976068, 165.28732557]],\n",
              "\n",
              "        [[157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [230.75887499, 204.00976068, 165.28732557],\n",
              "         ...,\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766]],\n",
              "\n",
              "        [[157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         ...,\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         ...,\n",
              "         [ 69.50700865,  62.57702058,  55.79331942],\n",
              "         [ 69.50700865,  62.57702058,  55.79331942],\n",
              "         [ 69.50700865,  62.57702058,  55.79331942]],\n",
              "\n",
              "        [[157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         ...,\n",
              "         [ 69.50700865,  62.57702058,  55.79331942],\n",
              "         [ 69.50700865,  62.57702058,  55.79331942],\n",
              "         [ 69.50700865,  62.57702058,  55.79331942]],\n",
              "\n",
              "        [[157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         [157.84720708, 135.70374569, 114.27113766],\n",
              "         ...,\n",
              "         [ 69.50700865,  62.57702058,  55.79331942],\n",
              "         [ 69.50700865,  62.57702058,  55.79331942],\n",
              "         [ 69.50700865,  62.57702058,  55.79331942]]],\n",
              "\n",
              "\n",
              "       [[[226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         ...,\n",
              "         [150.60053033, 127.02563268, 102.74278936],\n",
              "         [150.60053033, 127.02563268, 102.74278936],\n",
              "         [150.60053033, 127.02563268, 102.74278936]],\n",
              "\n",
              "        [[150.60053033, 127.02563268, 102.74278936],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         ...,\n",
              "         [150.60053033, 127.02563268, 102.74278936],\n",
              "         [150.60053033, 127.02563268, 102.74278936],\n",
              "         [150.60053033, 127.02563268, 102.74278936]],\n",
              "\n",
              "        [[226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         ...,\n",
              "         [150.60053033, 127.02563268, 102.74278936],\n",
              "         [ 61.77748847,  55.28641571,  49.77659574],\n",
              "         [150.60053033, 127.02563268, 102.74278936]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         ...,\n",
              "         [ 61.77748847,  55.28641571,  49.77659574],\n",
              "         [ 61.77748847,  55.28641571,  49.77659574],\n",
              "         [ 61.77748847,  55.28641571,  49.77659574]],\n",
              "\n",
              "        [[226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         ...,\n",
              "         [ 61.77748847,  55.28641571,  49.77659574],\n",
              "         [ 61.77748847,  55.28641571,  49.77659574],\n",
              "         [ 61.77748847,  55.28641571,  49.77659574]],\n",
              "\n",
              "        [[226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         [226.34289277, 197.81204883, 155.99054994],\n",
              "         ...,\n",
              "         [ 61.77748847,  55.28641571,  49.77659574],\n",
              "         [ 61.77748847,  55.28641571,  49.77659574],\n",
              "         [ 61.77748847,  55.28641571,  49.77659574]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(\".JPG\") or filename.endswith(\".png\"):\n",
        "        img = cv2.imread(os.path.join(image_folder, filename))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        img = cv2.resize(img, (224, 224))  # Resize images to a consistent size\n",
        "        images.append(img)\n",
        "\n",
        "# Step 2: K-Means clustering for enhanced segmentation\n",
        "\n",
        "def kmeans_segmentation(image, num_clusters=3):\n",
        "    reshaped_img = image.reshape((-1, 3))\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(reshaped_img)\n",
        "    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
        "    segmented_img = segmented_img.reshape(image.shape)\n",
        "    return segmented_img\n",
        "\n",
        "segmented_images = [kmeans_segmentation(img) for img in images]\n",
        "\n",
        "# Step 3: Extract features and create super-feature vector\n",
        "\n",
        "def extract_features(image):\n",
        "    # Extract statistical features (mean, standard deviation)\n",
        "    mean_rgb = np.mean(image, axis=(0, 1))\n",
        "    std_rgb = np.std(image, axis=(0, 1))\n",
        "\n",
        "    # Extract geometric features (aspect ratio, area, etc.)\n",
        "    height, width, _ = image.shape\n",
        "    aspect_ratio = width / height\n",
        "    area = height * width\n",
        "\n",
        "    return np.concatenate((mean_rgb, std_rgb, [aspect_ratio, area]))\n",
        "\n",
        "feature_vectors = [extract_features(img) for img in segmented_images]\n",
        "\n",
        "# Step 4: Deep learning-based classification model for foreground tree detection\n",
        "\n",
        "# Assuming you have 3 images and their corresponding foreground labels (0 or 1)\n",
        "# Example: 1 for foreground tree detected, 0 for no foreground tree detected\n",
        "y_foreground = [1, 0, 1]  # One foreground tree detected, no foreground tree, one foreground tree\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_foreground, X_test_foreground, y_train_foreground, y_test_foreground = train_test_split(np.array(segmented_images), y_foreground, test_size=0.2, random_state=0)\n",
        "\n",
        "# Build a CNN model for foreground tree classification\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_foreground, y_train_foreground, epochs=10, batch_size=32, validation_data=(X_test_foreground, np.array(y_test_foreground)))\n",
        "\n",
        "\n",
        "# # Step 3: Extract features and create super-feature vector\n",
        "\n",
        "# def extract_features(image):\n",
        "#     # Extract statistical features (mean, standard deviation)\n",
        "#     mean_rgb = np.mean(image, axis=(0, 1))\n",
        "#     std_rgb = np.std(image, axis=(0, 1))\n",
        "\n",
        "#     # Extract geometric features (aspect ratio, area, etc.)\n",
        "#     height, width, _ = image.shape\n",
        "#     aspect_ratio = width / height\n",
        "#     area = height * width\n",
        "\n",
        "#     return np.concatenate((mean_rgb, std_rgb, [aspect_ratio, area]))\n",
        "\n",
        "# feature_vectors = [extract_features(img) for img in segmented_images]\n",
        "\n",
        "# # Step 4: Deep learning-based classification model for foreground tree detection\n",
        "\n",
        "# # Assuming you have a labeled dataset for foreground tree detection (X_foreground, y_foreground)\n",
        "# # Here, X_foreground represents the images and y_foreground represents the binary labels (0 or 1)\n",
        "\n",
        "# y_foreground = [ 1,0,1]  # One foreground tree detected, no foreground tree, one foreground tree\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train_foreground, X_test_foreground, y_train_foreground, y_test_foreground = train_test_split(np.array(segmented_images), y_foreground, test_size=0.2, random_state=0)\n",
        "\n",
        "# # Build a CNN model for foreground tree classification\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D((2, 2)))\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(X_train_foreground, y_train_foreground, epochs=10, batch_size=32, validation_data=(X_test_foreground, y_test_foreground))\n",
        "\n",
        "# Step 5: Regression-based crop estimation using extracted features\n",
        "\n",
        "# Assuming you have a dataset for crop yield estimation (X_regression, y_regression)\n",
        "# Here, X_regression represents the extracted feature vectors and y_regression represents the crop yield values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_regression, X_test_regression, y_train_regression, y_test_regression = train_test_split(np.array(feature_vectors), y_regression, test_size=0.2, random_state=0)\n",
        "\n",
        "# Train a RandomForestRegressor for crop yield estimation\n",
        "regressor = RandomForestRegressor(random_state=0)\n",
        "regressor.fit(X_train_regression, y_train_regression)\n",
        "\n",
        "# Evaluate the regression model\n",
        "y_pred_regression = regressor.predict(X_test_regression)\n",
        "mse_regression = mean_squared_error(y_test_regression, y_pred_regression)\n",
        "\n",
        "print(f\"Mean Squared Error for Crop Yield Estimation: {mse_regression}\")\n",
        "\n",
        "# Step 6: Crop yield estimation for new images\n",
        "\n",
        "new_image = cv2.imread('path_to_new_image.jpg')\n",
        "new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
        "segmented_new_image = kmeans_segmentation(new_image)\n",
        "new_features = extract_features(segmented_new_image)\n",
        "\n",
        "# Use the trained regression model to estimate crop yield\n",
        "estimated_yield = regressor.predict([new_features])\n",
        "\n",
        "print(f\"Estimated Crop Yield: {estimated_yield[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "u0IORsymU9qX",
        "outputId": "00700a18-ad82-46cc-d4fc-0970e9e0cc53"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9b82b49c5af2>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_foreground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_foreground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_foreground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_foreground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1083\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1084\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
          ]
        }
      ]
    }
  ]
}